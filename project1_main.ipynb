{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.0'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.0.min.js\", \"https://cdn.holoviz.org/panel/1.4.2/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p1002'>\n",
       "  <div id=\"fac8c850-2eac-49a6-b850-234cc3e8492f\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"a4c7c7d6-221b-4f9f-ba01-f1bd93c0ebc2\":{\"version\":\"3.4.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1002\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1003\",\"attributes\":{\"plot_id\":\"p1002\",\"comm_id\":\"422b0ca4b78e42bc8de555a2fc83c235\",\"client_comm_id\":\"c5f36ba5d59145c8b07612e64c68e879\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"a4c7c7d6-221b-4f9f-ba01-f1bd93c0ebc2\",\"roots\":{\"p1002\":\"fac8c850-2eac-49a6-b850-234cc3e8492f\"},\"root_ids\":[\"p1002\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading dependencies.\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import requests\n",
    "import time\n",
    "from api_key import ip_api_key\n",
    "from api_key import ip_api_key2\n",
    "from api_key import ip_api_key3\n",
    "from api_key import ip_api_key4\n",
    "from api_key import ip_api_key5\n",
    "from datetime import datetime\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import hvplot.pandas\n",
    "from holoviews.util.transform import lon_lat_to_easting_northing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>IP</th>\n",
       "      <th>City</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Capital</th>\n",
       "      <th>Country</th>\n",
       "      <th>Transaction Date and Time</th>\n",
       "      <th>Transaction Amount</th>\n",
       "      <th>...</th>\n",
       "      <th>Card Expiration Date</th>\n",
       "      <th>CVV Code (Hashed or Encrypted)</th>\n",
       "      <th>Transaction Response Code</th>\n",
       "      <th>Fraud Flag or Label</th>\n",
       "      <th>Previous Transactions</th>\n",
       "      <th>Transaction Source</th>\n",
       "      <th>IP Address</th>\n",
       "      <th>Device Information</th>\n",
       "      <th>User Account Information</th>\n",
       "      <th>Transaction Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0003c9a2-3e18-499f-8b1f-f6f20eecb83c</td>\n",
       "      <td>11.133.155.94</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>39.97883</td>\n",
       "      <td>-82.89573</td>\n",
       "      <td>North America</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>2021-03-28 12:02:22</td>\n",
       "      <td>4584.73</td>\n",
       "      <td>...</td>\n",
       "      <td>09/31</td>\n",
       "      <td>162753c27c8b32975a0edf5e89ab4ed8e2f06f02a182e0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3 or more</td>\n",
       "      <td>Online</td>\n",
       "      <td>11.133.155.94</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>yuvraj-22</td>\n",
       "      <td>Culpa sit eligendi vel eaque aperiam quo. Sint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001538cf-4b3c-4d81-9cce-fb74fa5c6427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-11 15:30:47</td>\n",
       "      <td>2895.00</td>\n",
       "      <td>...</td>\n",
       "      <td>01/25</td>\n",
       "      <td>1e68ed4e3d58a51096a7feea3947f40debf1fd9246ec97...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Online</td>\n",
       "      <td>102.205.87.49</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>ukhanna</td>\n",
       "      <td>Assumenda amet corporis consectetur asperiores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0016926c-d82a-4070-a0ad-cd1416674744</td>\n",
       "      <td>210.63.242.180</td>\n",
       "      <td>Taipei City</td>\n",
       "      <td>25.05437</td>\n",
       "      <td>121.60681</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>Taipei</td>\n",
       "      <td>2023-07-16 01:16:14</td>\n",
       "      <td>1492.48</td>\n",
       "      <td>...</td>\n",
       "      <td>12/31</td>\n",
       "      <td>56f4da26ed956730309fa1488611ee0f13b0ac95ebb1bc...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In-Person</td>\n",
       "      <td>210.63.242.180</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Officiis incidunt minima magnam. Explicabo exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001b0dd8-5d3b-401a-9093-2093b562e559</td>\n",
       "      <td>175.92.199.54</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>39.90499</td>\n",
       "      <td>116.40529</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>2021-12-08 15:57:04</td>\n",
       "      <td>1077.20</td>\n",
       "      <td>...</td>\n",
       "      <td>03/25</td>\n",
       "      <td>8b6cd7c429e83373dbd412f43d7422c0c4a127d93d0f2a...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3 or more</td>\n",
       "      <td>In-Person</td>\n",
       "      <td>175.92.199.54</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Qui quam ad ipsam ab atque ipsa. Qui tempore q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003014ef-45b3-4743-bc6f-adab9a91a037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-12 03:15:14</td>\n",
       "      <td>2470.89</td>\n",
       "      <td>...</td>\n",
       "      <td>01/30</td>\n",
       "      <td>533eb9a8909f614c351b65e6b8aba1ffc2890735ce9a8a...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3 or more</td>\n",
       "      <td>Online</td>\n",
       "      <td>205.60.134.240</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>wagletushar</td>\n",
       "      <td>Deserunt fugit impedit odio molestiae reiciend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Transaction ID              IP         City  \\\n",
       "0  0003c9a2-3e18-499f-8b1f-f6f20eecb83c   11.133.155.94     Columbus   \n",
       "1  001538cf-4b3c-4d81-9cce-fb74fa5c6427             NaN          NaN   \n",
       "2  0016926c-d82a-4070-a0ad-cd1416674744  210.63.242.180  Taipei City   \n",
       "3  001b0dd8-5d3b-401a-9093-2093b562e559   175.92.199.54      Beijing   \n",
       "4  003014ef-45b3-4743-bc6f-adab9a91a037             NaN          NaN   \n",
       "\n",
       "        Lat        Lng      Continent           Capital           Country  \\\n",
       "0  39.97883  -82.89573  North America  Washington, D.C.  Washington, D.C.   \n",
       "1       NaN        NaN            NaN               NaN               NaN   \n",
       "2  25.05437  121.60681           Asia            Taipei            Taipei   \n",
       "3  39.90499  116.40529           Asia           Beijing           Beijing   \n",
       "4       NaN        NaN            NaN               NaN               NaN   \n",
       "\n",
       "  Transaction Date and Time  Transaction Amount  ... Card Expiration Date  \\\n",
       "0       2021-03-28 12:02:22             4584.73  ...                09/31   \n",
       "1       2022-06-11 15:30:47             2895.00  ...                01/25   \n",
       "2       2023-07-16 01:16:14             1492.48  ...                12/31   \n",
       "3       2021-12-08 15:57:04             1077.20  ...                03/25   \n",
       "4       2023-07-12 03:15:14             2470.89  ...                01/30   \n",
       "\n",
       "                      CVV Code (Hashed or Encrypted)  \\\n",
       "0  162753c27c8b32975a0edf5e89ab4ed8e2f06f02a182e0...   \n",
       "1  1e68ed4e3d58a51096a7feea3947f40debf1fd9246ec97...   \n",
       "2  56f4da26ed956730309fa1488611ee0f13b0ac95ebb1bc...   \n",
       "3  8b6cd7c429e83373dbd412f43d7422c0c4a127d93d0f2a...   \n",
       "4  533eb9a8909f614c351b65e6b8aba1ffc2890735ce9a8a...   \n",
       "\n",
       "  Transaction Response Code  Fraud Flag or Label Previous Transactions  \\\n",
       "0                         0                    1             3 or more   \n",
       "1                        12                    0                     2   \n",
       "2                         5                    1                     2   \n",
       "3                        12                    1             3 or more   \n",
       "4                        12                    0             3 or more   \n",
       "\n",
       "  Transaction Source      IP Address Device Information  \\\n",
       "0             Online   11.133.155.94             Mobile   \n",
       "1             Online   102.205.87.49            Desktop   \n",
       "2          In-Person  210.63.242.180             Tablet   \n",
       "3          In-Person   175.92.199.54             Tablet   \n",
       "4             Online  205.60.134.240            Desktop   \n",
       "\n",
       "  User Account Information                                  Transaction Notes  \n",
       "0                yuvraj-22  Culpa sit eligendi vel eaque aperiam quo. Sint...  \n",
       "1                  ukhanna  Assumenda amet corporis consectetur asperiores...  \n",
       "2                      NaN  Officiis incidunt minima magnam. Explicabo exp...  \n",
       "3                      NaN  Qui quam ad ipsam ab atque ipsa. Qui tempore q...  \n",
       "4              wagletushar  Deserunt fugit impedit odio molestiae reiciend...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to csv.\n",
    "card_fraud_path_load = Path(\"resource/merged_file.csv\")\n",
    "card_data = pd.read_csv(card_fraud_path_load)\n",
    "\n",
    "card_fraud_ernie_path_load = Path(\"resource/credit_card_fraud_flg_ernie.csv\")\n",
    "card_data_ernie = pd.read_csv(card_fraud_ernie_path_load)\n",
    "\n",
    "card_fraud_thet_path_load = Path(\"resource/credit_card_fraud_flg_thet.csv\")\n",
    "card_data_thet = pd.read_csv(card_fraud_thet_path_load)\n",
    "\n",
    "card_fraud_jimmy_path_load = Path(\"resource/credit_card_fraud_flg_jimmy.csv\")\n",
    "card_data_jimmy = pd.read_csv(card_fraud_jimmy_path_load)\n",
    "\n",
    "card_fraud_mounika_path_load = Path(\"resource/credit_card_fraud_flg_mounika.csv\")\n",
    "card_data_mounika = pd.read_csv(card_fraud_mounika_path_load)\n",
    "\n",
    "card_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ip_data(df, api_key):\n",
    "    # Base URL for the API\n",
    "    base_url = f'https://api.ipgeolocation.io/ipgeo?'\n",
    "    query_url_template = f\"{base_url}apiKey={api_key}&ip=\"\n",
    "    # Define an empty list to fetch the ipgeio data for each ip address\n",
    "    ip_data = []\n",
    "    # Print to logger\n",
    "    print(\"Beginning Data Retrieval     \")\n",
    "    print(\"-----------------------------\")\n",
    "    # Create counters\n",
    "    record_count = 1\n",
    "    set_count = 1\n",
    "    # Loop through all the ip addresses to fetch ip geo location data\n",
    "    for i, row in df.iterrows():\n",
    "        ip_address = row['IP Address']\n",
    "        transaction_id = row['Transaction ID']\n",
    "        # Group cities in sets of 50 for logging purposes\n",
    "        if (i % 50 == 0 and i >= 50):\n",
    "            set_count += 1\n",
    "            record_count = 0\n",
    "        # Log the url, record, and set numbers\n",
    "        print(f\"Processing Record {record_count} of Set {set_count}|{ip_address}\")\n",
    "        # Add 1 to the record count\n",
    "        record_count += 1\n",
    "        # Run an API request for each of the ip addresses\n",
    "        try:\n",
    "            query_url = f\"{query_url_template}{ip_address}\"\n",
    "            # Parse the JSON and retrieve data\n",
    "            ip_geo = requests.get(query_url).json()\n",
    "            # Parse out city, latitude, longitude, continent name, country capital and country name\n",
    "            ip_city = ip_geo.get('city','N/A')\n",
    "            ip_latitude = ip_geo.get('latitude','N/A')\n",
    "            ip_longitude = ip_geo.get('longitude','N/A')\n",
    "            ip_continent_name = ip_geo.get('continent_name','N/A')\n",
    "            ip_country_capital = ip_geo.get('country_capital','N/A')\n",
    "            ip_country_name = ip_geo.get('country_name','N/A')\n",
    "            # Append the City information into city_data list\n",
    "            ip_data.append({\n",
    "                \"Transaction ID\": transaction_id,\n",
    "                \"IP\": ip_address,\n",
    "                \"City\": ip_city,\n",
    "                \"Lat\": ip_latitude,\n",
    "                \"Lng\": ip_longitude,\n",
    "                \"Continent\": ip_continent_name,\n",
    "                \"Capital\": ip_country_capital,\n",
    "                \"Country\": ip_country_name\n",
    "            })\n",
    "        # If an error is experienced, skip the ip address\n",
    "        except requests.RequestException as e:\n",
    "            print(\"IP Address not found. Skipping...\")\n",
    "            continue\n",
    "    #Convert the list of dictionaries to a DataFrame\n",
    "    ip_data_df = pd.DataFrame(ip_data)\n",
    "    # Indicate that Data Loading is complete\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Data Retrieval Complete      \")\n",
    "    print(\"-----------------------------\")\n",
    "    return ip_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_data_thet = fetch_ip_data(card_data_thet, ip_api_key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_data_ernie = fetch_ip_data(card_data_ernie, ip_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_data_jimmy = fetch_ip_data(card_data_jimmy, ip_api_key3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_data_mounika = fetch_ip_data(card_data_mounika, ip_api_key4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ip_data_thet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m output_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/thet_ip_asn_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mip_data_thet\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(output_path, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m output_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/ernie_ip_asn_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m ip_data_ernie\u001b[38;5;241m.\u001b[39mto_csv(output_path, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ip_data_thet' is not defined"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"resource/thet_ip_asn_geolocation_data.csv\")\n",
    "ip_data_thet.to_csv(output_path, index = False)\n",
    "\n",
    "output_path = Path(\"resource/ernie_ip_asn_geolocation_data.csv\")\n",
    "ip_data_ernie.to_csv(output_path, index = False)\n",
    "\n",
    "output_path = Path(\"resource/jimmy_ip_asn_geolocation_data.csv\")\n",
    "ip_data_jimmy.to_csv(output_path, index = False)\n",
    "\n",
    "output_path = Path(\"resource/mounika_ip_asn_geolocation_data.csv\")\n",
    "ip_data_mounika.to_csv(output_path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resource\\\\ernie_ip_geolocation_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/ernie_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/jimmy_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/thet_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/mounika_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load each CSV file into a separate DataFrame\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(file_path) \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Combine all DataFrames into a single DataFrame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/ernie_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/jimmy_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/thet_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/mounika_ip_geolocation_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load each CSV file into a separate DataFrame\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Combine all DataFrames into a single DataFrame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Ernie\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ernie\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Ernie\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ernie\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Ernie\\anaconda3\\envs\\dev\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resource\\\\ernie_ip_geolocation_data.csv'"
     ]
    }
   ],
   "source": [
    "# Combined the four csv files for IP Geolocation Data \n",
    "# Define file paths\n",
    "file_paths = [\n",
    "    Path(\"resource/ernie_ip_geolocation_data.csv\"),\n",
    "    Path(\"resource/jimmy_ip_geolocation_data.csv\"),\n",
    "    Path(\"resource/thet_ip_geolocation_data.csv\"),\n",
    "    Path(\"resource/mounika_ip_geolocation_data.csv\")\n",
    "]\n",
    "\n",
    "# Load each CSV file into a separate DataFrame\n",
    "dataframes = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Optionally, save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(\"resource/combined_file.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m credit_card_fraud_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(credit_card_fraud_file_path)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Merge combined_df with credit_card_fraud_df on 'Transaction ID'\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\u001b[43mcombined_df\u001b[49m, credit_card_fraud_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransaction ID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m output_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresource/merged_file.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mto_csv(output_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Merge the combined_df into the merged_file\n",
    "# Load the credit card fraud data\n",
    "credit_card_fraud_file_path = Path(\"resource/credit_card_fraud.csv\")\n",
    "credit_card_fraud_df = pd.read_csv(credit_card_fraud_file_path)\n",
    "\n",
    "# Merge combined_df with credit_card_fraud_df on 'Transaction ID'\n",
    "merged_df = pd.merge(combined_df, credit_card_fraud_df, on='Transaction ID', how='inner')\n",
    "\n",
    "output_path = Path(\"resource/merged_file.csv\")\n",
    "merged_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_data=card_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   Transaction ID                           8000 non-null   object \n",
      " 1   IP                                       3989 non-null   object \n",
      " 2   City                                     3980 non-null   object \n",
      " 3   Lat                                      3989 non-null   float64\n",
      " 4   Lng                                      3989 non-null   float64\n",
      " 5   Continent                                3989 non-null   object \n",
      " 6   Capital                                  3989 non-null   object \n",
      " 7   Country                                  3989 non-null   object \n",
      " 8   Transaction Date and Time                8000 non-null   object \n",
      " 9   Transaction Amount                       8000 non-null   float64\n",
      " 10  Cardholder Name                          8000 non-null   object \n",
      " 11  Card Number (Hashed or Encrypted)        8000 non-null   object \n",
      " 12  Merchant Name                            8000 non-null   object \n",
      " 13  Merchant Category Code (MCC)             8000 non-null   int64  \n",
      " 14  Transaction Location (City or ZIP Code)  8000 non-null   object \n",
      " 15  Transaction Currency                     8000 non-null   object \n",
      " 16  Card Type                                8000 non-null   object \n",
      " 17  Card Expiration Date                     8000 non-null   object \n",
      " 18  CVV Code (Hashed or Encrypted)           8000 non-null   object \n",
      " 19  Transaction Response Code                8000 non-null   int64  \n",
      " 20  Fraud Flag or Label                      8000 non-null   int64  \n",
      " 21  Previous Transactions                    5957 non-null   object \n",
      " 22  Transaction Source                       8000 non-null   object \n",
      " 23  IP Address                               8000 non-null   object \n",
      " 24  Device Information                       8000 non-null   object \n",
      " 25  User Account Information                 3990 non-null   object \n",
      " 26  Transaction Notes                        8000 non-null   object \n",
      "dtypes: float64(3), int64(3), object(21)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# General info on data.\n",
    "card_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3989 fraud or flag or labeled transactions in the dataset.\n",
      "There are 4011 valid transactions in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Counted how many (potential) fraud or valid transactions there are in the data.\n",
    "fraud_data = card_data[card_data['Fraud Flag or Label'] == 1]\n",
    "fraud_data_count = len(fraud_data)\n",
    "valid_data_row = card_data[card_data['Fraud Flag or Label'] == 0]\n",
    "valid_data_count = len(valid_data_row)\n",
    "print(f\"There are {fraud_data_count} fraud or flag or labeled transactions in the dataset.\")\n",
    "print(f\"There are {valid_data_count} valid transactions in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_data['Cardholder Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardholder names that has fraud or flag or labeled transactions. (more than 1 means multiple transactions.)\n",
    "fraudulent_cardholder = fraud_data.groupby(['Cardholder Name']).size()\n",
    "fraudulent_cardholder.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if there is any null value in the column.\n",
    "card_data['Transaction Date and Time'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatted the column into a date form.\n",
    "card_data_transaction_date = pd.to_datetime(card_data['Transaction Date and Time'], format ='%Y-%m-%d %H:%M:%S')\n",
    "card_data_expiration_date = pd.to_datetime(card_data['Card Expiration Date'], format='%m/%y')\n",
    "card_data[\"Merchant Category Code (MCC)\"]=card_data[\"Merchant Category Code (MCC)\"].astype(int)\n",
    "card_data_transaction_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the fraudulent transactions data\n",
    "flagged_data = card_data[card_data['Fraud Flag or Label'] == 1].copy()\n",
    "\n",
    "# Convert the 'Transaction Date and Time' column to datetime\n",
    "flagged_data['Transaction Date and Time'] = pd.to_datetime(fraud_data['Transaction Date and Time'], format ='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Extract the month and hour from the transaction date\n",
    "flagged_data['Month'] = flagged_data['Transaction Date and Time'].dt.month\n",
    "flagged_data['Hour'] = flagged_data['Transaction Date and Time'].dt.hour\n",
    "\n",
    "# Group by month & hour and count the number of transactions\n",
    "flagged_transactions_per_month = flagged_data.groupby('Month').size()\n",
    "flagged_transactions_per_hour = flagged_data.groupby('Hour').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample group by with Month and transaction ID\n",
    "flagged_transactions_per_month_id = flagged_data.groupby(['Month', 'Transaction ID']).size()\n",
    "flagged_transactions_per_month_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperated all the date data into 'year', 'month', 'day'.\n",
    "card_data_transaction_year = card_data_transaction_date.dt.year\n",
    "card_data_transaction_month = card_data_transaction_date.dt.month\n",
    "card_data_transaction_day = card_data_transaction_date.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for transaction date distribution.\n",
    "plt.hist(card_data_transaction_date, bins=50, edgecolor='white')\n",
    "plt.title(\"Transactions date distribution\")\n",
    "plt.xlabel(\"transaction_date\")\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created new columns regarding year.\n",
    "transactions_per_year = card_data.groupby(card_data_transaction_year).size()\n",
    "transactions_per_year = transactions_per_year.reset_index()\n",
    "transactions_per_year.columns = ['Year', 'Transaction per year']\n",
    "transactions_per_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created bar graph to show transactions per year.\n",
    "x_axis = transactions_per_year['Year']\n",
    "y_axis = transactions_per_year['Transaction per year']\n",
    "\n",
    "tick_locations = []\n",
    "\n",
    "for x in x_axis:\n",
    "    tick_locations.append(x)\n",
    "\n",
    "plt.bar(x_axis, y_axis, color=\"darkblue\", alpha=0.9, align=\"center\", width=0.55)\n",
    "plt.xticks(tick_locations, rotation=90, ha='center')\n",
    "plt.title(\"# of transactions per year \")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"# of Transactions\")\n",
    "plt.savefig('output/Fig1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can easily see that the transactions through credit card has significantly decreased on the year '2023', we believe that the after math of covid-19 has carried over until the 2020 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created new columns regarding month.\n",
    "transactions_per_month = card_data.groupby(card_data_transaction_month).size()\n",
    "transactions_per_month = transactions_per_month.reset_index()\n",
    "transactions_per_month.columns = ['Month', 'Transactions per month']\n",
    "transactions_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created bar graph to show transactions per month. (1 = January)\n",
    "x_axis = transactions_per_month['Month']\n",
    "y_axis = transactions_per_month['Transactions per month']\n",
    "\n",
    "plt.plot(x_axis, y_axis, color=\"darkblue\", alpha=0.9)\n",
    "plt.title(\"# of transactions per month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"# of Transactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the yearly transaction graph, we dig little deeper and look at the trends on monthly transactions. Surprisingly, the amount of transactions from users significantly dips on the September and going into the end of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of fraudulent transactions per month\n",
    "x_axis = flagged_transactions_per_month.index\n",
    "y_axis = flagged_transactions_per_month.values\n",
    "\n",
    "plt.plot(x_axis, y_axis, color=\"darkblue\", alpha=0.9)\n",
    "plt.title(\"# of Flagged Transactions per Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"# of Flagged Transactions\")\n",
    "plt.savefig('output/Fig2.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created bar graph\n",
    "x_axis = flagged_transactions_per_hour.index\n",
    "y_axis = flagged_transactions_per_hour.values\n",
    "\n",
    "plt.bar(x_axis, y_axis, alpha=0.9)\n",
    "plt.title(\"# of Flagged Transactions per Hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"# of Flagged Transactions\")\n",
    "plt.savefig('output/Fig3.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on this bar graph we can see that 5:00am, 10:00am, 7:00pm has the highest flagged transactions; therefore, as a credit card company they need to staff the customer support agents to appropriate time slot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data to get the total count of fraudulent transactions per month\n",
    "fraud_transactions_per_month_id_0 = flagged_transactions_per_month_id.groupby(level=0).count()\n",
    "\n",
    "x_axis = fraud_transactions_per_month_id_0.index\n",
    "y_axis = fraud_transactions_per_month_id_0.values\n",
    "\n",
    "# Perform linear regression\n",
    "(slope, intercept, r_value, p_value, std_err) = linregress(x_axis, y_axis)\n",
    "\n",
    "# Create equation of line to calculate predicted fraudulent transaction\n",
    "line_eq = \"y = \" + str(round(slope,2)) + \"x + \" + str(round(intercept,2))\n",
    "\n",
    "# Generate a scatter plot of fraudulent transactions over months\n",
    "plt.scatter(x_axis, y_axis, alpha=0.9)\n",
    "plt.title(\"# of Flagged Transactions per Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"# of Flagged Transactions\")\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(x_axis, intercept + slope*x_axis, 'r', label='fitted line')\n",
    "\n",
    "# Annotate the plot with the linear equation\n",
    "plt.annotate(line_eq, xy=(0.15, 0.35), xycoords='axes fraction', fontsize=15, color=\"red\")\n",
    "\n",
    "print(f\"The r_value is: {r_value}\")\n",
    "plt.savefig('output/Fig4.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The r-value of -0.74 suggest that there is a strong negative correlation between the month (x-axis) and the number of flagged transactions (y-axis). This means that as time progresses, the number of flagged transactions tends to decrease. For each month that passes, the model predicts approximately 7.93 fewer flagged transactions. However, this is a simple model and doesn’t prove causation, so other factors could also be influencing the decrease in flagged transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange rates to USD (example rates, replace with actual rates)\n",
    "exchange_rates = {\n",
    "    \"USD\": 1.0,    # USD to USD\n",
    "    \"EUR\": 1.2,    # 1 EUR = 1.2 USD\n",
    "    \"INR\": 0.013   # 1 INR = 0.013 USD\n",
    "}\n",
    "\n",
    "# Function to convert amounts to USD\n",
    "def convert_to_usd(amount, currency):\n",
    "    return amount * round(exchange_rates[currency],2)\n",
    "\n",
    "# Apply conversion to the dataframe\n",
    "flagged_data[\"Amount in USD\"] = flagged_data.apply(lambda row: convert_to_usd(row[\"Transaction Amount\"], row[\"Transaction Currency\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average transaction amount for each category and sort as per the provided parameters\n",
    "card_type_avg = flagged_data.groupby(\"Card Type\")[\"Amount in USD\"].mean().sort_values(ascending=False).reset_index()\n",
    "device_type_avg = flagged_data.groupby(\"Device Information\")[\"Amount in USD\"].mean().sort_values(ascending=False).reset_index()\n",
    "currency_type_avg = flagged_data.groupby(\"Transaction Currency\")[\"Amount in USD\"].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.5\n",
    "\n",
    "# Set bar positions for each category with a small gap between categories\n",
    "gap = 1\n",
    "r1 = np.arange(len(card_type_avg))\n",
    "r2 = np.arange(len(device_type_avg)) + len(card_type_avg) + gap\n",
    "r3 = np.arange(len(currency_type_avg)) + len(card_type_avg) + len(device_type_avg) + 2 * gap\n",
    "\n",
    "# Plot bars\n",
    "bars1 = plt.bar(r1, card_type_avg[\"Amount in USD\"], color=\"skyblue\", width=bar_width, edgecolor=\"grey\", label=\"Card Type\", alpha=0.5)\n",
    "bars2 = plt.bar(r2, device_type_avg[\"Amount in USD\"], color=\"yellow\", width=bar_width, edgecolor=\"grey\", label=\"Device Information\", alpha=0.5)\n",
    "bars3 = plt.bar(r3, currency_type_avg[\"Amount in USD\"], color=\"green\", width=bar_width, edgecolor=\"grey\", label=\"Transaction Currency\", alpha=0.5)\n",
    "\n",
    "# Add labels for x-axis\n",
    "all_positions = list(r1) + list(r2) + list(r3)\n",
    "all_labels = list(card_type_avg[\"Card Type\"]) + list(device_type_avg[\"Device Information\"]) + list(currency_type_avg[\"Transaction Currency\"])\n",
    "\n",
    "plt.xlabel(\"Category\", fontsize=14)\n",
    "plt.xticks(all_positions, all_labels, rotation=45, ha='right', fontsize=12)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Average Amount in USD by Category\", fontsize=16)\n",
    "plt.ylabel(\"Average Amount in USD\", fontsize=14)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend()\n",
    "\n",
    "# Adding text labels inside the bars\n",
    "for bar in bars1:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "for bar in bars2:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "for bar in bars3:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('output/Fig5.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating subplots for 3 categories vs transxn amount\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Box Plot for Card Type\n",
    "axes[0].boxplot([group[\"Amount in USD\"].values for name, group in flagged_data.groupby('Card Type')])\n",
    "axes[0].set_title('Box Plot of Amount in USD by Card Type',fontsize=14)\n",
    "axes[0].set_xticklabels(flagged_data['Card Type'].unique(), rotation=45, ha='right',fontsize=14)\n",
    "\n",
    "# Box Plot for Device Information\n",
    "axes[1].boxplot([group[\"Amount in USD\"].values for name, group in flagged_data.groupby('Device Information')])\n",
    "axes[1].set_title('Box Plot of Amount in USD by Device Information',fontsize=14)\n",
    "axes[1].set_xticklabels(flagged_data['Device Information'].unique(), rotation=45, ha='right',fontsize=14)\n",
    "\n",
    "# Box Plot for Transaction Currency\n",
    "axes[2].boxplot([group[\"Amount in USD\"].values for name, group in flagged_data.groupby('Transaction Currency')])\n",
    "axes[2].set_title('Box Plot of Amount in USD by Transaction Currency',fontsize=14)\n",
    "axes[2].set_xticklabels(flagged_data['Transaction Currency'].unique(), rotation=45, ha='right',fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA using statsmodels\n",
    "# Combine all categorical columns into a long format for ANOVA\n",
    "df_long = pd.melt(flagged_data, id_vars=['Amount in USD'], value_vars=['Card Type', 'Device Information', 'Transaction Currency'],\n",
    "                  var_name='Category', value_name='Type')\n",
    "\n",
    "# Perform ANOVA using statsmodels\n",
    "model = ols('Q(\"Amount in USD\") ~ C(Type)', data=df_long).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(anova_table)\n",
    "\n",
    "# Perform ANOVA using scipy.stats (for each category separately)\n",
    "card_type_anova = f_oneway(*[group[\"Amount in USD\"].values for name, group in flagged_data.groupby('Card Type')])\n",
    "device_type_anova = f_oneway(*[group[\"Amount in USD\"].values for name, group in flagged_data.groupby('Device Information')])\n",
    "currency_type_anova = f_oneway(*[group[\"Amount in USD\"].values for name, group in flagged_data.groupby('Transaction Currency')])\n",
    "\n",
    "print(\"ANOVA results for Card Type:\", card_type_anova)\n",
    "print(\"ANOVA results for Device Information:\", device_type_anova)\n",
    "print(\"ANOVA results for Transaction Currency:\", currency_type_anova)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA Results Interpretation:\n",
    "Card Type:\n",
    "\n",
    "\n",
    "p-value: 0.025\n",
    "Interpretation:\n",
    "\n",
    "The p-value (0.025) is less than the typical significance level of 0.05, indicating that there are statistically significant differences in the average transaction amounts across different Card Types.\n",
    "\n",
    "Device Information:\n",
    "p-value: 0.316\n",
    "Interpretation:\n",
    "\n",
    "The p-value (0.316) is greater than 0.05, indicating that there is no statistically significant difference in the average transaction amounts across different Device Information categories.\n",
    "\n",
    "\n",
    "Transaction Currency:\n",
    "p-value: 0.0\n",
    "Interpretation:\n",
    "\n",
    "The p-value (0.0) is much less than 0.05, indicating a highly significant difference in the average transaction amounts across different Transaction Currencies.\n",
    "\n",
    "Conclusion:\n",
    "Card Type: There are significant differences in average transaction         amounts between different card types.\n",
    "\n",
    "Device Information: There are no significant differences in average transaction amounts between different device types.\n",
    "\n",
    "Transaction Currency: There are highly significant differences in average transaction amounts between different currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_counts = flagged_data[\"Transaction Source\"].value_counts()\n",
    "source_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "The data shows a nearly even split between In-Person and Online transactions.\n",
    "In-Person transactions are slightly higher by a margin of 59 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing Merchant Category Codes and thier occurences\n",
    "mcc_counts =flagged_data[\"Merchant Category Code (MCC)\"].value_counts()\n",
    "mcc_counts=mcc_counts.reset_index()\n",
    "mcc_counts.columns=[\"Mcc\",\"Counts\"]\n",
    "filtered_mcc=mcc_counts.loc[mcc_counts[\"Counts\"]==4]\n",
    "\n",
    "#Providing description to the MCC\n",
    "bins = [0, 1500, 3000, 3300, 3500, 4000, 4800, 5000, 5700, 7300, 8000, 9000, 10000]\n",
    "descriptions = [\n",
    "    \"Agricultural services\",\n",
    "    \"Contracted services\",\n",
    "    \"Airlines\",\n",
    "    \"Car rentals\",\n",
    "    \"Lodging\",\n",
    "    \"Transportation services\",\n",
    "    \"Utility services\",\n",
    "    \"Retail outlet services\",\n",
    "    \"Miscellaneous stores\",\n",
    "    \"Business services\",\n",
    "    \"Professional services and membership organizations\",\n",
    "    \"Government services\"\n",
    "]\n",
    "#Adding Description column to MCC\n",
    "final_mcc = filtered_mcc.copy()\n",
    "final_mcc.loc[:, \"Description\"] = pd.cut(filtered_mcc[\"Mcc\"], bins, labels=descriptions, include_lowest=True)\n",
    "final_mcc\n",
    "\n",
    "#Filtering MCC codes with max occurence of 4\n",
    "grouped_df = final_mcc.groupby('Description',observed=True)[\"Counts\"].sum().reset_index()\n",
    "filtered_df = grouped_df[grouped_df['Counts'] >= 4]\n",
    "\n",
    "#Sorting MCC by their number of occurences\n",
    "filtered_df = filtered_df.reset_index(drop=True).sort_values(by='Counts', ascending=False)\n",
    "print(filtered_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This portion of the analysis contains observing Capital and the average transaction amounts where fraudulent flag equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique number of capital in the database\n",
    "flagged_data['Capital'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on Country\n",
    "card_data_dedup = flagged_data.drop_duplicates(subset=['Capital'])\n",
    "\n",
    "# Check that the filtered data frame is the same size as the number of unique countries\n",
    "card_data_dedup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the Dataframe by Country\n",
    "card_data_dedup = card_data_dedup.sort_values('Capital')\n",
    "\n",
    "# Set country as index\n",
    "#card_data_dedup.set_index('Country', inplace=True)\n",
    "\n",
    "# Display Sample Data\n",
    "card_data_dedup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out what are the top countries number of fruads occured \n",
    "flagged_data['Capital'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean value of Transaction Amount Per Country\n",
    "avg_txn_amt_by_cntry = flagged_data.groupby('Capital', as_index=False)['Transaction Amount'].mean()\n",
    "avg_txn_amt_by_cntry = avg_txn_amt_by_cntry.sort_values('Capital')\n",
    "avg_txn_amt_by_cntry['Transaction Amount'] = round(avg_txn_amt_by_cntry['Transaction Amount'],2)\n",
    "\n",
    "# Display sample data and check that it has the same number of rows as the dedup table\n",
    "avg_txn_amt_by_cntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dedup dataframe and the fraud transaction mean dataframe\n",
    "card_data_dedup_txn = pd.merge(card_data_dedup, avg_txn_amt_by_cntry, how=\"left\", on=['Capital'])\n",
    "\n",
    "# Rename the new merged column to Avg. Transaction Amount\n",
    "card_data_dedup_txn.rename(columns={'Transaction Amount_y': 'Avg. Transaction Amount'}, inplace=True)\n",
    "\n",
    "# Reset the index\n",
    "card_data_dedup_txn.reset_index()\n",
    "\n",
    "# Display sample data\n",
    "card_data_dedup_txn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns for Map - Capital, Lat, Lng, Avg transact Amount, Continent, \n",
    "map_df = card_data_dedup_txn[['Capital', 'Lat', 'Lng', 'Avg. Transaction Amount', 'Continent']]\n",
    "\n",
    "# Display sample data\n",
    "map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the coordinates to Web Mercator\n",
    "map_df['x'], map_df['y'] = lon_lat_to_easting_northing(map_df.Lng, map_df.Lat)\n",
    "\n",
    "# Display sample data\n",
    "map_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The following image is derived by calling \n",
    " ip Geo location API (web source:  https://app.ipgeolocation.io/) using the IP Address provided in the data source.\n",
    " The API uses the IP address passed as an argument and returns information such as the Latitude, Longtitude, Country Name, \n",
    " Country's Capital and Continent which we will use for analysis.\n",
    "\n",
    "\n",
    "Based on the geo location map below, we observe that most of the credit card transactions where Fraud Flag = 1 take place in Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Minimum and Maximum Avg. Transaction Amounts\n",
    "print(\"Max Ave. Transaction Amount: \" + str(max(map_df['Avg. Transaction Amount'])))\n",
    "print(\"Min Ave. Transaction Amount: \" + str(min(map_df['Avg. Transaction Amount'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges for marker size to plot\n",
    "bins = [0, 1000, 2000, 3000, 4000, 5000]\n",
    "\n",
    "# The labels list will be used for plotting - Marker size\n",
    "labels = ['50', '100', '150', '200', '250']\n",
    "map_df['Marker'] = pd.cut(map_df['Avg. Transaction Amount'], bins=bins, labels=labels)\n",
    "\n",
    "# Convert the Dataframe into numeric for Marker size\n",
    "map_df['Marker'] = pd.to_numeric(map_df['Marker'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot map - Fraudulent Transactions by Capital\n",
    "map_plot_1 = map_df.hvplot.points(\n",
    "    'x', \n",
    "    'y', \n",
    "    tiles = True,\n",
    "    frame_width = 800,\n",
    "    frame_height = 600, \n",
    "    alpha=0.7,\n",
    "    size = \"Marker\",\n",
    "    color = 'Capital',\n",
    "    hover_cols = ['Capital', 'Avg. Transaction Amount', 'Continent'],\n",
    "    title='Flagged Transactions by Capital'\n",
    "    )\n",
    "\n",
    "map_plot_1\n",
    "#hvplot.save(map_plot_1, 'output/Fig6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The average fraudulent transaction amount was calculated and charted against latitue to see if there are any correlations between the two. There does not appear to be any significat correlation between these 2 measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the scatter plots for latitude vs. average fradulent transaction amount\n",
    "x_values = card_data_dedup_txn['Lat']\n",
    "y_values = card_data_dedup_txn['Avg. Transaction Amount']\n",
    "\n",
    "# Plot the scatter plot with datapoints edge folor = black and opicity=90%\n",
    "plt.scatter(x_values,y_values,alpha=0.9, edgecolors='black')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Average Flagged Transaction Amount')\n",
    "plt.grid('on')\n",
    "\n",
    "# Show plot\n",
    "plt.savefig('output/Fig7.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count for each continent\n",
    "map_df['Continent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "The Pie chart below helps us visualize the distribution of flagged activities between continents. As previously note, Europe had the most activities followed by Asia and Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pie plot showing the distribution of Continents with fraudulent activities\n",
    "\n",
    "# Get the labels and values \n",
    "seriesLabels_pie = ['Europe','Asia','Africa','North America','South America','Oceania']\n",
    "seriesValues_pie = map_df['Continent'].value_counts()\n",
    "\n",
    "# Draw bar chart\n",
    "plt.pie(seriesValues_pie, labels=seriesLabels_pie, autopct='%1.0f%%',startangle=180)\n",
    "\n",
    "# Set the labels\n",
    "plt.title(\"Distribution of Continents with Flagged Activities\")\n",
    "\n",
    "# Show the pie chart\n",
    "plt.savefig('output/Fig8.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
